{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a85ac44e-50d4-4ca4-8eca-d5f98cc767fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Supply Chain Anomaly Detection Demo\n",
    "    \n",
    "This notebook demonstrates the complete workflow of the Supply Chain Anomaly Detection system using a sample dataset. It walks through:\n",
    "\n",
    "1. Creating sample data\n",
    "2. Data preprocessing and feature engineering,\n",
    "3. Anomaly detection,\n",
    "4. Issue classification,\n",
    "5. Recommendation generation,\n",
    "6. Visualization and analysis,\n",
    "\n",
    "Let's start by importing necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.conda (Python 3.11.13)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n .conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!conda install -n .conda ipykernel --update-deps --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "893671c1-7e7e-4fef-8fa9-b1e07656e484",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.conda (Python 3.11.13)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n .conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Upgrade NumPy to the latest version\n",
    "%pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8ba7965-9bed-4c18-b010-aeba69b70400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8492ef00-c3fe-4331-ae73-a3ed83e92eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the project root to Python path to import our package\n",
    "# This assumes the notebook is in the notebooks/ directory\n",
    "sys.path.append(\"../supply-chain-anomaly-detection\")\n",
    "\n",
    "# Configure plot style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Import our main class\n",
    "from src.models.sc_issue_detection import SupplyChainIssueDetection\n",
    "\n",
    "# Optional: Import MLflow utilities if available\n",
    "try:\n",
    "    from mlflow import mlflow_utils\n",
    "    MLFLOW_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MLFLOW_AVAILABLE = False\n",
    "    print(\"MLflow not available. Install with 'pip install mlflow' for experiment tracking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21c41bdd-3260-488f-ad3e-753f0a3043ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9723d865-7b0a-4f59-bac4-d8db63544533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1. Generate Sample Data\n",
    "\n",
    "First, let's create a sample dataset for supply chain metrics. We'll create a dataset with some known anomalies to demonstrate the detection capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "348ca4cb-870b-44df-83ec-107e3f5754a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_sample_data(n_records=50, seed=42):\n",
    "    \"\"\"Create a sample dataset with supply chain metrics.\"\"\"\n",
    "    np.random.seed(seed)  # For reproducibility\n",
    "    \n",
    "    # Define sample values for categorical features\n",
    "    skus = [f\"SKU_{i}\" for i in range(1, 6)]\n",
    "    countries = ['US', 'UK', 'DE']\n",
    "    quarters = ['2024Q1', '2024Q2']\n",
    "    \n",
    "    # Create base data\n",
    "    data = pd.DataFrame({\n",
    "        'SKU': np.random.choice(skus, n_records),\n",
    "        'Country': np.random.choice(countries, n_records),\n",
    "        'Quarter': np.random.choice(quarters, n_records),\n",
    "        'SellThru': np.random.randint(100, 1000, n_records),\n",
    "        'SellTo': np.random.randint(80, 950, n_records),\n",
    "        'T2Inventory': np.random.randint(200, 2000, n_records),\n",
    "        'DistributorInventory': np.random.randint(100, 1500, n_records),\n",
    "        'Backlog': np.random.randint(0, 500, n_records),\n",
    "        'Shipments': np.random.randint(50, 800, n_records),\n",
    "        'AgedInventory': np.random.randint(0, 300, n_records),\n",
    "        'WeeksOfStockT1': np.random.uniform(1, 10, n_records),\n",
    "        'WeeksOfStockT2': np.random.uniform(2, 12, n_records),\n",
    "        'NumCompetitors': np.random.randint(1, 10, n_records),\n",
    "        'PricePositioning': np.random.uniform(80, 120, n_records),\n",
    "        'TargetQty': np.random.randint(200, 1200, n_records)\n",
    "    })\n",
    "    \n",
    "    # Introduce some anomalies (around 10%)\n",
    "    anomaly_indices = np.random.choice(n_records, size=5, replace=False)\n",
    "    \n",
    "    # Inventory imbalances\n",
    "    data.loc[anomaly_indices[0], 'WeeksOfStockT1'] = 15\n",
    "    \n",
    "    # Sales performance gaps\n",
    "    data.loc[anomaly_indices[1], 'SellTo'] = int(data.loc[anomaly_indices[1], 'TargetQty'] * 0.4)\n",
    "    \n",
    "    # Pricing issues\n",
    "    data.loc[anomaly_indices[2], 'PricePositioning'] = 130\n",
    "    \n",
    "    # Supply chain disruptions\n",
    "    data.loc[anomaly_indices[3], 'Backlog'] = 800\n",
    "    \n",
    "    # Sell-through bottlenecks\n",
    "    data.loc[anomaly_indices[4], 'SellThru'] = int(data.loc[anomaly_indices[4], 'SellTo'] * 0.5)\n",
    "    \n",
    "    return data\n",
    "    \n",
    "# Create the sample data\n",
    "sample_data = create_sample_data(n_records=50)\n",
    "\n",
    "# Create directory for data if it doesn't exist\n",
    "os.makedirs('/Workspace/Repos/mohammed.jeddi@hp.com/supply-chain-anomaly-detection/src/data/sample/', exist_ok=True)\n",
    "\n",
    "# Save the sample data\n",
    "sample_data_path = '/Workspace/Repos/mohammed.jeddi@hp.com/supply-chain-anomaly-detection/src/data/sample/sample_supply_chain_data.csv'\n",
    "sample_data.to_csv(sample_data_path, index=False)\n",
    "\n",
    "print(f\"Sample data created with shape: {sample_data.shape}\")\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae4fcc30-3b46-4fed-96c8-a95b4514072d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Initialize and Configure the Supply Chain Issue Detection System\n",
    "\n",
    "Now, let's initialize our anomaly detection system. We'll configure it with parameters suitable for our sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f345322c-bfed-4bc9-a6ea-ea4bac7653a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the detector\n",
    "# Note: Set use_llm=True if you have an OpenAI API key and want LLM-enhanced recommendations\n",
    "detector = SupplyChainIssueDetection(\n",
    "    use_llm=False,  # Set to True if you have an OpenAI API key in environment\n",
    "    contamination=0.3,  # We expect about 30% anomalies in our sample data\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "print(\"Supply Chain Issue Detection system initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e37e1ded-d777-4101-a86b-7274c1b41254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Start MLflow Tracking (Optional)\n",
    "\n",
    "If MLflow is available, we'll use it to track our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71ec5c51-6eaf-4eab-8ca8-078ba3a5a23b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start MLflow tracking if available\n",
    "if MLFLOW_AVAILABLE:\n",
    "    mlflow_run = mlflow_utils.start_run(experiment_name=\"supply_chain_demo\")\n",
    "    \n",
    "    # Log parameters\n",
    "    params = {\n",
    "        'contamination': 0.1,\n",
    "        'random_state': 42,\n",
    "        'use_llm': False,\n",
    "        'data_source': 'sample_data'\n",
    "    }\n",
    "    mlflow_utils.log_parameters(params)\n",
    "    \n",
    "    print(f\"MLflow tracking started with run ID: {mlflow_run.info.run_id}\")\n",
    "else:\n",
    "    print(\"MLflow tracking not available. Continuing without experiment tracking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "958ae97c-8032-47a8-95bf-d075e128379f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Data Processing Pipeline\n",
    "\n",
    "Now, let's run through the entire pipeline steps using our sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "150cecc6-bc97-4909-9c29-617058814c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.1 Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c64f755-904f-4074-a425-91c5b9bd617b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = detector.load_data(text)\n",
    "print(f\"Loaded data with shape: {data.shape}\")\n",
    "\n",
    "# Preprocess data\n",
    "processed_data = detector.preprocess_data()\n",
    "print(f\"Preprocessed data with shape: {processed_data.shape}\")\n",
    "\n",
    "# Show engineered features\n",
    "engineered_features = [\n",
    "    'SellThruToRatio', \n",
    "   'InventoryTurnoverRate', \n",
    "   'TargetAchievement',\n",
    "   'TargetAchievement_ship', \n",
    "   'SupplyChainEfficiency', \n",
    "    'AgedInventoryPct'\n",
    "]\n",
    "\n",
    "processed_data[engineered_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c81b6f71-9b99-435d-8a85-42b5d027d280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.2 Detect Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be19dcac-ab5d-45fe-9c3c-3a2586647590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train anomaly detector\n",
    "detector.train_anomaly_detector()\n",
    "\n",
    "# Count anomalies\n",
    "anomaly_count = detector.data['is_anomaly'].sum()\n",
    "print(f\"Detected {anomaly_count} anomalies out of {len(detector.data)} records ({anomaly_count/len(detector.data)*100:.1f}%)\")\n",
    "\n",
    "# Look at model agreement\n",
    "agreement_df = pd.DataFrame({\n",
    "    'Isolation Forest': detector.data['if_anomaly'],\n",
    "    'LOF': detector.data['lof_anomaly'] if 'lof_anomaly' in detector.data.columns else np.zeros(len(detector.data)),\n",
    "    'One-Class SVM': detector.data['ocsvm_anomaly'],\n",
    "   'Ensemble Decision': detector.data['is_anomaly'],\n",
    "   'Anomaly Score': detector.data['anomaly_score']\n",
    "})\n",
    "\n",
    "# Display anomalies\n",
    "anomalies_df = detector.data[detector.data['is_anomaly'] == 1].copy()\n",
    "anomalies_df[['Global_Distributor_Group_Name','reporter_name', 'reporter_hq_id', 'PRODUCT_GROUP_BMT','fiscal_year_quarter', 'product_number', 'reporter_country_code', 'SellThruToRatio', \n",
    "            'InventoryTurnoverRate', \n",
    "            'TargetAchievement',\n",
    "            'TargetAchievement_ship', \n",
    "            'SupplyChainEfficiency', \n",
    "            'AgedInventoryPct', 'anomaly_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c56e74-2ca0-4526-98b7-18f05c1154db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.3 Visualize Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cf99ba2-f144-45f5-bce7-6f36a0db6751",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Key Insights:\n",
    "- Critical stock-outs (<1 week) are associated with poor target achievement\n",
    "- Overstocking (>7 weeks) indicates potential forecasting issues\n",
    "- Optimal inventory levels fall between 2-5 weeks of stock\n",
    "- Clustering of anomalies reveals distinct supply chain problem patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb074075-cb39-4841-9e81-8622fd119b8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize anomalies by WeeksOfStock vs TargetAchievement\n",
    "detector.visualize_anomalies('t1_wos', 'TargetAchievement')\n",
    "\n",
    "# Visualize anomalies by SellThruToRatio vs InventoryTurnoverRate\n",
    "detector.visualize_anomalies('SellThruToRatio', 'InventoryTurnoverRate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e0138fa-0fa7-49e5-a4b3-80a62158c468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.4 Classify Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ecf3bd6-a994-4248-8d9b-3ca9e38bfe98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Classify issues\n",
    "classified_anomalies = detector.classify_issues(use_ml=True)\n",
    "\n",
    "# Display issue distribution\n",
    "issue_counts = classified_anomalies['issue_type'].value_counts()\n",
    "print(\"Issue Type Distribution:\")\n",
    "print(issue_counts)\n",
    "\n",
    "# Plot issue distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "issue_counts.plot(kind='bar', color='teal')\n",
    "plt.title('Distribution of Issue Types')\n",
    "plt.xlabel('Issue Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display classified anomalies\n",
    "classified_anomalies[['Global_Distributor_Group_Name','reporter_name', 'reporter_hq_id', 'PRODUCT_GROUP_BMT','fiscal_year_quarter', 'product_number', 'reporter_country_code','issue_type', 'anomaly_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "603bbccb-19a3-4cb9-b191-3e56a7832623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.5 Generate Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34887e9f-cfe0-42ec-bda2-8b14abd3d369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate recommendations\n",
    "recommendations = detector.generate_recommendations(classified_anomalies)\n",
    "\n",
    "# Display recommendations\n",
    "recommendations[['Global_Distributor_Group_Name','reporter_name', 'reporter_hq_id', 'PRODUCT_GROUP_BMT','fiscal_year_quarter', 'product_number', 'reporter_country_code', 'Issue_Type', 'Priority', 'Final_Recommendation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d30f5d9b-c116-4043-a79f-be0d4158c81a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c070aad8-9d19-4623-b263-ab8f7f07a94f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.6 PCA Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4b0395d-28bf-4088-850c-8a5404d18e98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "detector.data=recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a286b58d-9cb2-413a-aa53-6f483de60ade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Visualize using PCA\n",
    "pca, pca_anomalies = detector.visualize_with_pca()\n",
    "\n",
    "# Analyze PCA results\n",
    "pca_analysis = detector.analyze_pca_results(pca, pca_anomalies)\n",
    "\n",
    "# Print key insights\n",
    "print(f\"Total explained variance: {pca_analysis['total_explained_variance']:.2f}%\")\n",
    "if pca_analysis['silhouette_score'] is not None:\n",
    "    print(f\"Silhouette score: {pca_analysis['silhouette_score']:.3f}\")\n",
    "print(f\"Separation quality: {pca_analysis['separation_quality']}\")\n",
    "\n",
    "print(\"\\nTop features driving categorization:\")\n",
    "top_features = sorted(pca_analysis['feature_importance'], key=lambda x: x['Total_Importance'], reverse=True)[:5]\n",
    "for feature in top_features:\n",
    "    print(f\"  - {feature['Feature']}: {feature['Total_Importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bed11aa7-6ab1-4772-a176-a628d377a3cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dc4a888-02f6-42bf-bb33-de471ccae38c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "path=''\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "model_path = path +'/demo_supply_chain_model'\n",
    "detector.save_models(model_path)\n",
    "print(f\"Models saved to {model_path}\")\n",
    "\n",
    "# Save recommendations to CSV\n",
    "os.makedirs('../supply-chain-anomaly-detection/data/processed', exist_ok=True)\n",
    "recommendations.to_csv('../supply-chain-anomaly-detection/data/processed/demo_recommendations.csv', index=False)\n",
    "print(\"Recommendations saved to ../data/processed/demo_recommendations.csv\")\n",
    "\n",
    "# Log to MLflow if available\n",
    "if MLFLOW_AVAILABLE:\n",
    "    # Log metrics\n",
    "    metrics = {\n",
    "        'anomaly_count': anomaly_count,\n",
    "        'anomaly_percentage': anomaly_count/len(detector.data)*100\n",
    "    }\n",
    "    \n",
    "    # Add issue type counts\n",
    "    for issue, count in issue_counts.items():\n",
    "        metrics[f'issue_count_{issue}'] = count\n",
    "    \n",
    "    # Add PCA metrics if available\n",
    "    if pca_analysis:\n",
    "        metrics['pca_explained_variance'] = pca_analysis['total_explained_variance']\n",
    "        if pca_analysis['silhouette_score'] is not None:\n",
    "            metrics['silhouette_score'] = pca_analysis['silhouette_score']\n",
    "    \n",
    "    mlflow_utils.log_metrics(metrics)\n",
    "    \n",
    "    # Log recommendations as artifact\n",
    "    mlflow_utils.log_dataframe(recommendations, 'recommendations', 'csv')\n",
    "    \n",
    "    # Log model\n",
    "    mlflow_utils.log_model(detector, \"supply_chain_detector\")\n",
    "    \n",
    "    print(\"Results logged to MLflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1437ee48-fa98-4cf8-92ef-a6d88fc8efda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Test Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17ebfe5a-e134-44c8-b15c-1e4229e04d1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the saved model\n",
    "loaded_detector = SupplyChainIssueDetection.load_models(model_path)\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# Test on new data - we'll use the same data for demo purposes\n",
    "# In real scenarios, you would use new, unseen data\n",
    "data, new_recommendations = loaded_detector.process_new_data(sample_data_path)\n",
    "\n",
    "print(f\"Processed new data: Found {len(new_recommendations)} anomalies\")\n",
    "new_recommendations[['SKU', 'Country', 'issue_type', 'Final_Recommendation']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d0c838f-4639-44a1-a323-5fc1fc39e819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expected_features = loaded_detector.preprocessor.numerical_features\n",
    "expected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2d59531-3a34-4863-bd5b-81c7844cf75b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. End MLflow Run (if started)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a891c0b4-d8af-4e11-bf79-6dc8a68f8d9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# End MLflow run if it was started\n",
    "if MLFLOW_AVAILABLE and 'mlflow_run' in locals():\n",
    "    mlflow_run.__exit__(None, None, None)\n",
    "    print(\"MLflow run ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd40770d-b7f5-459c-965b-92da212219fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Summary and Conclusions\n",
    "\n",
    "In this notebook, we've demonstrated the complete workflow of the Supply Chain Anomaly Detection system:\n",
    "\n",
    "1. **Data Processing**: We loaded and preprocessed supply chain data, creating engineered features to improve anomaly detection.\n",
    "\n",
    "2. **Anomaly Detection**: Using an ensemble approach that combines multiple algorithms (Isolation Forest, LOF, One-Class SVM), we identified anomalous patterns in the data.\n",
    "\n",
    "3. **Issue Classification**: We categorized the detected anomalies into specific issue types such as Inventory Imbalance, Sales Performance Gap, Pricing Issue, etc.\n",
    "\n",
    "4. **Recommendation Generation**: For each anomaly, we generated tailored recommendations to address the underlying issues.\n",
    "\n",
    "5. **Visualization and Analysis**: We created visualizations to understand anomalies and their patterns, and used PCA to validate issue categorization quality.\n",
    "\n",
    "6. **Model Management**: We saved the models for future use and demonstrated how to load them for processing new data.\n",
    "\n",
    "7. **MLflow Integration**: We showed how MLflow can be used to track experiments, metrics, and models.\n",
    "\n",
    "This end-to-end workflow demonstrates the capabilities of the Supply Chain Anomaly Detection system for identifying and addressing issues in supply chain operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f13102f2-1d64-43ee-8b53-835c22fc2d7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_supply_chain_anomaly_detection_demo",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
